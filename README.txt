Web Scraping Project
Огляд Проєкту
Цей проєкт є модульним та розширюваним веб-скрапером, розробленим для збору даних про продукти з вказаних категорій на веб-сайті. Він побудований з акцентом на чисту архітектуру, принципи об'єктно-орієнтованого програмування (ООП) та легкість конфігурації.

Ключові Особливості:
Модульна та Розширювана Архітектура: Використання абстрактних базових класів (ABC) та Dependency Injection дозволяє легко замінювати компоненти (наприклад, веб-скрапер, екстрактор, менеджер бази даних) без зміни основної логіки.

Конфігурація через Змінні Середовища: Більшість ключових параметрів конфігурації керуються через змінні середовища, що забезпечує гнучкість та безпеку.

Примітка: Деякі параметри (наприклад, HEADERS) захардкоджені в config.py і потребуватимуть зміни коду для оновлення.

Обробка Помилок та Логування: Вбудована система логування для відстеження роботи застосунку та обробки потенційних проблем.

Примітка: Пряме перетворення типів (int()) для змінних середовища може призвести до помилок під час виконання, якщо змінні відсутні або мають некоректний формат.

Висока Тестованість: Архітектура розроблена таким чином, щоб зробити компоненти легко тестованими (навіть якщо юніт-тести не є обов'язковими для всіх модулів).

Структура Проєкту
Проєкт організовано в чітку, ієрархічну структуру для кращої модульності та читабельності:

your_project_name/
├── .env                  # Змінні середовища (конфіденційні та конфігуровані)
├── main.py               # Точка входу в програму, ініціалізація Orchestrator
├── config.py             # Завантаження та валідація конфігурації з .env
└── src/                  # Основний код програми
    ├── __init__.py       # Робить 'src' Python-пакетом
    ├── core/             # Абстракції та основні інтерфейси
    │   ├── __init__.py
    │   ├── abstract_scraper.py
    │   ├── abstract_extractor.py
    │   └── abstract_database_manager.py
    ├── implementation/   # Конкретні реалізації інтерфейсів
    │   ├── __init__.py
    │   ├── web_scraper.py
    │   ├── product_extractor.py
    │   └── database.py
    └── orchestration/    # Логіка координації
        ├── __init__.py
        ├── orchestrator.py
        └── workers.py


Встановлення та Запуск
Передумови
Python 3.8+

pip (менеджер пакетів Python)

PostgreSQL (або інша сумісна база даних, якщо ви плануєте змінити реалізацію DatabaseManager)

1. Клонування Репозиторію
Спершу клонуйте репозиторій на ваш локальний комп'ютер:

git clone https://github.com/MykheiSilchuk/WebScrapingProject.git
cd WebScrapingProject


2. Встановлення Залежностей
Створіть віртуальне середовище (рекомендується) та встановіть необхідні бібліотеки.
Вам потрібно створити файл requirements.txt у кореневій директорії вашого проєкту, перелічивши всі бібліотеки, які використовує ваш проєкт. Наприклад:

python-dotenv
requests
os
logging
abc
time
urllib.parse
queue
threading
concurrent.futures
lxml
psycopg2-binary # Якщо ви використовуєте PostgreSQL


Після створення requirements.txt, виконайте:

python -m venv venv
# Для Windows:
.\venv\Scripts\activate
# Для macOS/Linux:
source venv/bin/activate

pip install -r requirements.txt


3. Налаштування Змінних Середовища
Створіть файл .env у кореневій директорії проєкту (поруч з main.py). Цей файл не буде відслідковуватися Git і використовуватиметься для зберігання конфіденційної або специфічної для середовища конфігурації.

# .env - Example configuration
DB_HOST=localhost
DB_NAME=your_database_name
DB_USER=your_user
DB_PASSWORD=your_password
DB_PORT=5432

SCRAPER_BASE_URL=https://www.example.com # Замініть на реальний URL цільового сайту
SCRAPER_NEEDED_CATEGORIES="devops,it-infrastructure,data-analytics-and-management" # Розділені комами назви категорій

SCRAPER_TEST_MODE=true            # Встановіть false для звичайного режиму
TEST_PRODUCT_LIMIT=10             # Ліміт продуктів для скрапінгу в тестовому режимі
SLEEP_BETWEEN_CATEGORY_PAGES=3    # Затримка між обходом сторінок категорій (секунди)
SLEEP_BETWEEN_PRODUCT_PAGES=1     # Затримка між обходом сторінок продуктів (секунди)

Важливо: Заповніть ці значення відповідно до вашого налаштування бази даних та цільового веб-сайту. Переконайтеся, що всі змінні, які використовуються в config.py, присутні у вашому .env файлі, щоб уникнути помилок під час виконання.

4. Запуск Проєкту
Після налаштування віртуального середовища та .env файлу, ви можете запустити скрапер:

python main.py


Конфігурація
Конфігураційні параметри визначені у файлі config.py та завантажуються зі змінних середовища (файлу .env).

DB_HOST, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT: Налаштування для підключення до бази даних.

SCRAPER_BASE_URL: Базовий URL веб-сайту для скрапінгу.

SCRAPER_NEEDED_CATEGORIES: Список категорій, розділених комами.

SCRAPER_TEST_MODE: Булеве значення (true/false) для активації тестового режиму.

TEST_PRODUCT_LIMIT: Максимальна кількість продуктів для скрапінгу в тестовому режимі.

HEADERS: Захардкоджений словник HTTP-заголовків у config.py.

SLEEP_BETWEEN_CATEGORY_PAGES: Затримка між запитами сторінок категорій.

SLEEP_BETWEEN_PRODUCT_PAGES: Затримка між запитами сторінок продуктів.

Ліцензія
Цей проєкт поширюється під вказати ліцензію, наприклад MIT License.
